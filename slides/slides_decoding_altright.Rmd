---
title: "Decoding the Alt-Right"
subtitle: "A Machine Learning Project"
author: "Simon & Fabio"
date: "2018/05/10"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, echo = F, include=F}
# include=FALSE
options(htmltools.dir.version = FALSE)

pacman::p_load(dplyr, ggplot2, googlesheets, openxlsx, stringr, rvest, dplyr, ggplot2, keras, mlrMBO, tidyMBO, ggthemes, Smisc, randomForest, parallelMap, emoa)

knitr::opts_chunk$set(echo = F, warning = F, error = F, message = F)
```

## Outline

TODO:

* Repo Setup
* Homepage
* Intearctive plots
* QR Code/Bitly
* Hero Banner: Banner + Ãœbergang in Charlottsville torches march/metalic
* DecodingTheAltRight.eu


1. Theoretical Setting
    * What is the alt-right?
2. Data Collection
    * Where are they?
    * Who did we scrape?
    * wheach media outlets? logo
    * descriptives
    * Timerange
3. Coding Framework
    * Coding Scheme
    * App
    * Voluntary Coders (tree)
    * Reliability and Validation
    * Descripives of labeled data
4. Machine Learning
    * Train/Test Split
    * RNN/LSTM/MLP language models 
    * `tidyMBO`
    * t-SNE word embedding
    * Performance tables
5. Predictive Analysis
    * Terror
    * Hoax: Narrative. Coparative Narratives. Frist occurance. Charelltsvill Quebec Mosque shooter. 
    * Media over time
    * ALt-Right figures over time. 
6. Policy Implications
    * Counter Narratives
    * AI deletion: What about uncertainty and ambiguity of language.
    * Human in the loop. 
    * Feeback loop: get personal feedback. Restcricted times.
    * Algorithemn must be repsonsive and transparent on how decisions are made. Highlight problematic content and a reason why. 
    * Cross Checking Algorithemns improve the comprehension of how the system works and we get a chance to critize its blindspots. 
    * More accountable AI programming. 

---
class: inverse, center, middle

## Get Started


---

## Fallbacks

* Keyword counting
* Official racesim dictionaries.
* Network analysis.
    + Link share network
    + user community
* Gender
* Terror
* Story tracing/hoax
  




---

## Who did we scrape?

```{r data, echo = F, eval = F}
googlesheets::gs_auth(token = "shiny_app_token.rds")
with_label <- gs_title("altright_data_final") %>%
  gs_read()

clean_slider <- function(x){
  x %>%
    str_replace_all("Not Present", "1") %>%
    str_replace_all("Strongly Present", "5") %>%
    str_replace_all("99", "0")
}

df_coded <- with_label %>%
  filter(!duplicated(text)) %>%
  arrange(id) %>%
  purrr::map_df(clean_slider) %>%
  mutate_at(vars(identity:anti_mus), as.numeric)

dt <- df_coded %>%
  #dplyr::select(identity:left, anti_fem:anti_mus) %>%
  #purrr::map_df(.f = ~ifelse(.x == 1, 1, 2)) %>%
  #cbind(., text = df_coded$text) %>%
  mutate(text = as.character(text))

#save(dt, file = "data/dt.Rdata")
```




```{r tally, echo = F, fig.width = 10, fig.height=7, fig.align="center"}
load("data/dt.Rdata")
dt %>%
  mutate(pl1 = as.numeric(as.factor(platform))) %>%
  group_by(platform, pl1, page) %>%
  tally %>%
  ungroup %>%
  mutate(page = forcats::fct_reorder(page, pl1)) %>%
  mutate(platform = case_when(
      platform == "fb" ~ "Facebook",
      platform == "tw" ~ "Twitter",
      platform == "yt" ~ "YouTube"
    )
  ) %>%
  filter(!is.na(page)) %>%
  arrange(platform, desc(n)) %>%
  ggplot(aes(page, platform, fill = n)) +
  geom_tile() +
  coord_flip() +
  theme_hc() +
  theme(legend.position = "right", text = element_text(size = 20, face = "bold")) +
  viridis::scale_fill_viridis("Number", option = "E", alpha = .5, direction = -1) +
  labs(x = "", y = "") +
  scale_y_discrete(position = "top")
```





---












